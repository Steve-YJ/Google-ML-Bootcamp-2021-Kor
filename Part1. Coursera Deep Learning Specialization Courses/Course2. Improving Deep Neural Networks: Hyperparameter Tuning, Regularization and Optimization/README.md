# Course2. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
> In the second course of the Deep Learning Specialization, you will open the deep learning black box to understand the processes that drive performance and generate good results systematically. 
* learn best practices to train and develop test sets  
* analyze bias/variance for building deep learning applications
* be able to use standard neural network techniques such as initialization, L2 and dropout regularization, hyperparameter tuning, batch normalization, gradient checking
* implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence; 
* implement a neural network in TensorFlow.
<br>

## Week1. Practical Aspects of Deep Learning
> Discover and experiment with a variety of different initialization methods, apply L2 regularization and dropout to avoid model overfitting, then apply gradient checking to identify errors in a fraud detection model.
### Goal
* Give examples of how different types of initializations can lead to different results
* Examine the importance of initialization in complex neural networks
* Explain the difference between train/dev/test sets
* Diagnose the bias and variance issues in your model
* Assess the right time and place for using regularization methods such as dropout or L2 regularization
* Explain Vanishing and Exploding gradients and how to deal with them
* Use gradient checking to verify the accuracy of your backpropagation implementation
* Apply zeros initialization, random initialization, and He initialization
* Apply regularization to a deep learning model

### Lessons Learned
- [ ] Quiz. Practical aspects of Deep Learning
- [ ] Programming Assignment: Initialization
- [ ] Programming Assignment: Regularization
- [ ] Programming Assignment: Gradient Checking
<br>

## Week2. Optimization Algorithms
> Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate decay scheduling to speed up your models.
### Goal
* Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
* Use random minibatches to accelerate convergence and improve optimization
* Describe the benefits of learning rate decay and apply it to your optimization

### Lessons Learned
- [ ] Quiz. Optimization Algorithms
- [ ] Programming Assignment: Optimization Methods

<br>
## Week3. Hyperparameter Tuning, Batch Normalization and Programming Frameworks
> Explore TensorFlow, a deep learning framework that allows you to build neural networks quickly and easily, then train a neural network on a TensorFlow dataset.
### Goal
* Master the process of hyperparameter tuning
* Describe softmax classification for multiple classes
* Apply batch normalization to make your neural network more robust
* Build a neural network in TensorFlow and train it on a TensorFlow dataset
* Describe the purpose and operation of GradientTape
* Use tf.Variable to modify the state of a variable
* Apply TensorFlow decorators to speed up code
* Explain the difference between a variable and a constant
### Lessons Learned
- [ ] Quiz: Hyperparameter tuning, Batch Normalization, Programming Frameworks
- [ ] Programming Assignment: TensorFlow Introduction

<br>

# Next: Learn TensorFlow
> It's time to learn TensorFlow!

- [ ] TensorFlow: Data and Deployment Specialized Course [link](https://www.coursera.org/specializations/tensorflow-data-and-deployment?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=TF1)
